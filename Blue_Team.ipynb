{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Deepak/anaconda/lib/python3.6/site-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "#importing\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import numpy as np\n",
    "import pickle as cPickle\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.use('Agg')\n",
    "#from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Definitions\n",
    "# Convert into correct type for theano\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "# Weights are shared theano variables\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n",
    "\n",
    "# RMSProp to update weights\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "# Dropout regularization \n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "# Neural network model, 3 fully connected layers\n",
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "\t# Input layer: dropout + relu \n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = T.nnet.relu(T.dot(X, w_h))\n",
    "\n",
    "\t# Hidden layer: dropout + relu \n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = T.nnet.relu(T.dot(h, w_h2))\n",
    "\t\n",
    "\t# Output layer: dropout + softmax \n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    py_x = T.nnet.softmax(T.dot(h2, w_o))\n",
    "    return h, h2, py_x\n",
    "\n",
    "def one_hot(x,n):\n",
    "\tif type(x) == list:\n",
    "\t\tx = np.array(x)\n",
    "\tx = x.flatten()\n",
    "\to_h = np.zeros((len(x),n))\n",
    "\to_h[np.arange(len(x)),x] = 1\n",
    "\treturn o_h\n",
    "\n",
    "def mnist(ntrain=60000,ntest=10000,onehot=True):\n",
    "\tdata_dir = os.path.join(datasets_dir,'mnist/')\n",
    "\tfd = open(os.path.join(data_dir,'train-images-idx3-ubyte'))\n",
    "\tloaded = np.fromfile(file=fd,dtype=np.uint8)\n",
    "\ttrX = loaded[16:].reshape((60000,28*28)).astype(float)\n",
    "\n",
    "\tfd = open(os.path.join(data_dir,'train-labels-idx1-ubyte'))\n",
    "\tloaded = np.fromfile(file=fd,dtype=np.uint8)\n",
    "\ttrY = loaded[8:].reshape((60000))\n",
    "\n",
    "\tfd = open(os.path.join(data_dir,'t10k-images-idx3-ubyte'))\n",
    "\tloaded = np.fromfile(file=fd,dtype=np.uint8)\n",
    "\tteX = loaded[16:].reshape((10000,28*28)).astype(float)\n",
    "\n",
    "\tfd = open(os.path.join(data_dir,'t10k-labels-idx1-ubyte'))\n",
    "\tloaded = np.fromfile(file=fd,dtype=np.uint8)\n",
    "\tteY = loaded[8:].reshape((10000))\n",
    "\n",
    "\ttrX = trX/255.\n",
    "\tteX = teX/255.\n",
    "\n",
    "\ttrX = trX[:ntrain]\n",
    "\ttrY = trY[:ntrain]\n",
    "\n",
    "\tteX = teX[:ntest]\n",
    "\tteY = teY[:ntest]\n",
    "\n",
    "\tif onehot:\n",
    "\t\ttrY = one_hot(trY, 10)\n",
    "\t\tteY = one_hot(teY, 10)\n",
    "\telse:\n",
    "\t\ttrY = np.asarray(trY)\n",
    "\t\tteY = np.asarray(teY)\n",
    "\n",
    "\treturn trX,teX,trY,teY\n",
    "\n",
    "def plot_mnist_digit(image1, image2, name1, name2):\n",
    "    global count_attack\n",
    "    image1 = np.reshape(image1,[1,784])\n",
    "    image2 = np.reshape(image2,[1,784])\n",
    "    #print 'test image confidence' , np.mean(predict_conf(image1)), 'adversarial image confidence', np.mean(predict_conf(image2))\n",
    "    if (predict(image1) != predict(image2)):\n",
    "        count_attack = count_attack + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST data Loaded\n"
     ]
    }
   ],
   "source": [
    "#Loading MNIST data\n",
    "datasets_dir = 'media/datasets/'\n",
    "srng = RandomStreams()\n",
    "TRAINING = True\n",
    "\n",
    "print('MNIST data Loaded')\n",
    "trX, teX, trY, teY = mnist(onehot=True)\n",
    "\n",
    "# Initialize theano variables for X, Y, and shared variables for weights\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "if TRAINING:\n",
    "    # For training of the net, we initialize weights to random values\n",
    "    w_h = init_weights((784, 625))\n",
    "    w_h2 = init_weights((625, 625))\n",
    "    w_o = init_weights((625, 10))\n",
    "    params = [w_h, w_h2, w_o]\n",
    "else:\n",
    "    # To run experiments, just read weights we learned before\n",
    "    print('Loading model...')\n",
    "    with open('LearnedParamsL1_2.model','rb') as fp:\n",
    "        params = cPickle.load(fp)\n",
    "    w_h, w_h2, w_o = params\n",
    "\n",
    "# Dropout model for training\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, w_h2, w_o, 0.2, 0.5)\n",
    "# Use all-weights model for prediction\n",
    "h, h2, py_x = model(X, w_h, w_h2, w_o, 0., 0.)\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "# To find confidence of test set use the following value of y_x\n",
    "y_x1 = T.max(py_x, axis = 1)\n",
    "# Define cost and update theano expressions\n",
    "\n",
    "l1 = abs(w_h).sum() + abs(w_h2).sum() + abs(w_o).sum()\n",
    "l2 = (w_h**2).sum() + (w_h2**2).sum() + (w_o**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1coef = 0.000000, l2coef = 0.000000\n",
      "Training MNIST data...\n",
      "0 :   0.9816\n",
      "1 :   0.9831\n",
      "2 :   0.9824\n",
      "3 :   0.9824\n",
      "4 :   0.9833\n",
      "5 :   0.983\n",
      "6 :   0.9824\n",
      "7 :   0.9836\n",
      "8 :   0.983\n",
      "9 :   0.9841\n",
      "10 :   0.9838\n",
      "11 :   0.9832\n",
      "12 :   0.9847\n",
      "13 :   0.9847\n",
      "14 :   0.9835\n",
      "15 :   0.9836\n",
      "16 :   0.9839\n",
      "17 :   0.9836\n",
      "18 :   0.9823\n",
      "19 :   0.9849\n",
      "20 :   0.9845\n",
      "21 :   0.9843\n",
      "22 :   0.9831\n",
      "23 :   0.9839\n",
      "24 :   0.9837\n",
      "25 :   0.9839\n",
      "26 :   0.9842\n",
      "27 :   0.9842\n",
      "28 :   0.9839\n",
      "29 :   0.985\n",
      "30 :   0.9844\n",
      "31 :   0.9837\n",
      "32 :   0.9832\n",
      "33 :   0.9845\n",
      "34 :   0.984\n",
      "35 :   0.9845\n",
      "36 :   0.9838\n",
      "37 :   0.9833\n",
      "38 :   0.9846\n",
      "39 :   0.9846\n",
      "40 :   0.9839\n",
      "41 :   0.9847\n",
      "42 :   0.9839\n",
      "43 :   0.9846\n",
      "44 :   0.9845\n",
      "45 :   0.9858\n",
      "46 :   0.9847\n",
      "47 :   0.9849\n",
      "48 :   0.9857\n",
      "49 :   0.9846\n",
      "Accuracy is:     0.9846\n",
      "Confidence is:     0.992364662768\n",
      "l1coef = 0.000000, l2coef = 0.000010\n",
      "Training MNIST data...\n",
      "0 :   0.9836\n",
      "1 :   0.9856\n",
      "2 :   0.9843\n",
      "3 :   0.985\n",
      "4 :   0.9841\n",
      "5 :   0.9852\n",
      "6 :   0.985\n",
      "7 :   0.986\n",
      "8 :   0.9849\n",
      "9 :   0.9847\n",
      "10 :   0.9858\n",
      "11 :   0.9848\n",
      "12 :   0.9861\n",
      "13 :   0.9849\n",
      "14 :   0.9857\n",
      "15 :   0.9855\n",
      "16 :   0.9853\n",
      "17 :   0.9835\n",
      "18 :   0.9856\n",
      "19 :   0.9851\n",
      "20 :   0.9858\n",
      "21 :   0.9855\n",
      "22 :   0.9855\n",
      "23 :   0.9856\n",
      "24 :   0.9846\n",
      "25 :   0.9851\n",
      "26 :   0.9853\n",
      "27 :   0.9852\n",
      "28 :   0.986\n",
      "29 :   0.9849\n",
      "30 :   0.9855\n",
      "31 :   0.9859\n",
      "32 :   0.9853\n",
      "33 :   0.9854\n",
      "34 :   0.986\n",
      "35 :   0.9855\n",
      "36 :   0.9861\n",
      "37 :   0.9852\n",
      "38 :   0.9859\n",
      "39 :   0.9848\n",
      "40 :   0.9855\n",
      "41 :   0.9854\n",
      "42 :   0.9853\n",
      "43 :   0.985\n",
      "44 :   0.9858\n",
      "45 :   0.985\n",
      "46 :   0.9863\n",
      "47 :   0.9847\n",
      "48 :   0.9856\n",
      "49 :   0.9856\n",
      "Accuracy is:     0.9856\n",
      "Confidence is:     0.991601831842\n",
      "l1coef = 0.000000, l2coef = 0.000100\n",
      "Training MNIST data...\n",
      "0 :   0.9855\n",
      "1 :   0.9852\n",
      "2 :   0.9857\n",
      "3 :   0.9852\n",
      "4 :   0.9846\n",
      "5 :   0.9842\n",
      "6 :   0.9837\n",
      "7 :   0.9827\n",
      "8 :   0.9835\n",
      "9 :   0.9832\n",
      "10 :   0.9824\n",
      "11 :   0.9832\n",
      "12 :   0.9819\n",
      "13 :   0.9837\n",
      "14 :   0.9838\n",
      "15 :   0.9825\n",
      "16 :   0.9829\n",
      "17 :   0.9828\n",
      "18 :   0.9826\n",
      "19 :   0.9832\n",
      "20 :   0.9835\n",
      "21 :   0.9831\n",
      "22 :   0.9807\n",
      "23 :   0.9837\n",
      "24 :   0.9835\n",
      "25 :   0.9832\n",
      "26 :   0.9813\n",
      "27 :   0.9824\n",
      "28 :   0.9821\n",
      "29 :   0.9821\n",
      "30 :   0.9834\n",
      "31 :   0.9825\n",
      "32 :   0.9808\n",
      "33 :   0.9812\n",
      "34 :   0.9837\n",
      "35 :   0.9827\n",
      "36 :   0.9817\n",
      "37 :   0.9789\n",
      "38 :   0.9821\n",
      "39 :   0.9846\n",
      "40 :   0.9804\n",
      "41 :   0.9813\n",
      "42 :   0.9821\n",
      "43 :   0.983\n",
      "44 :   0.9831\n",
      "45 :   0.9819\n",
      "46 :   0.9811\n",
      "47 :   0.9824\n",
      "48 :   0.9822\n",
      "49 :   0.981\n",
      "Accuracy is:     0.981\n",
      "Confidence is:     0.98557561618\n",
      "l1coef = 0.000010, l2coef = 0.000000\n",
      "Training MNIST data...\n",
      "0 :   0.9823\n",
      "1 :   0.9841\n",
      "2 :   0.983\n",
      "3 :   0.9819\n",
      "4 :   0.9839\n",
      "5 :   0.9836\n",
      "6 :   0.9821\n",
      "7 :   0.9831\n",
      "8 :   0.9819\n",
      "9 :   0.981\n",
      "10 :   0.984\n",
      "11 :   0.9836\n",
      "12 :   0.9838\n",
      "13 :   0.9844\n",
      "14 :   0.9839\n",
      "15 :   0.9834\n",
      "16 :   0.9846\n",
      "17 :   0.9833\n",
      "18 :   0.984\n",
      "19 :   0.9821\n",
      "20 :   0.9839\n",
      "21 :   0.9846\n",
      "22 :   0.9834\n",
      "23 :   0.9842\n",
      "24 :   0.9828\n",
      "25 :   0.9825\n",
      "26 :   0.9844\n",
      "27 :   0.9842\n",
      "28 :   0.9835\n",
      "29 :   0.9847\n",
      "30 :   0.9839\n",
      "31 :   0.9844\n",
      "32 :   0.9848\n",
      "33 :   0.9827\n",
      "34 :   0.9836\n",
      "35 :   0.9858\n",
      "36 :   0.9831\n",
      "37 :   0.9844\n",
      "38 :   0.9854\n",
      "39 :   0.9852\n",
      "40 :   0.9835\n",
      "41 :   0.9832\n",
      "42 :   0.984\n",
      "43 :   0.9859\n",
      "44 :   0.9824\n",
      "45 :   0.9852\n",
      "46 :   0.9843\n",
      "47 :   0.9833\n",
      "48 :   0.9829\n",
      "49 :   0.9845\n",
      "Accuracy is:     0.9845\n",
      "Confidence is:     0.989788713909\n",
      "l1coef = 0.000010, l2coef = 0.000010\n",
      "Training MNIST data...\n",
      "0 :   0.9842\n",
      "1 :   0.9836\n",
      "2 :   0.9825\n",
      "3 :   0.9851\n",
      "4 :   0.9836\n",
      "5 :   0.9855\n",
      "6 :   0.9838\n",
      "7 :   0.9858\n",
      "8 :   0.9851\n",
      "9 :   0.9863\n",
      "10 :   0.9841\n",
      "11 :   0.9842\n",
      "12 :   0.9849\n",
      "13 :   0.9838\n",
      "14 :   0.9834\n",
      "15 :   0.9843\n",
      "16 :   0.984\n",
      "17 :   0.9843\n",
      "18 :   0.9843\n",
      "19 :   0.9832\n",
      "20 :   0.9843\n",
      "21 :   0.9845\n",
      "22 :   0.9852\n",
      "23 :   0.9844\n",
      "24 :   0.9841\n",
      "25 :   0.9848\n",
      "26 :   0.983\n",
      "27 :   0.9838\n",
      "28 :   0.9843\n",
      "29 :   0.9844\n",
      "30 :   0.9843\n",
      "31 :   0.983\n",
      "32 :   0.9842\n",
      "33 :   0.9853\n",
      "34 :   0.9836\n",
      "35 :   0.9847\n",
      "36 :   0.983\n",
      "37 :   0.9838\n",
      "38 :   0.9832\n",
      "39 :   0.9821\n",
      "40 :   0.984\n",
      "41 :   0.9851\n",
      "42 :   0.9855\n",
      "43 :   0.9836\n",
      "44 :   0.9855\n",
      "45 :   0.9831\n",
      "46 :   0.9835\n",
      "47 :   0.9834\n",
      "48 :   0.9848\n",
      "49 :   0.9838\n",
      "Accuracy is:     0.9838\n",
      "Confidence is:     0.986927274744\n",
      "l1coef = 0.000010, l2coef = 0.000100\n",
      "Training MNIST data...\n",
      "0 :   0.9846\n",
      "1 :   0.9837\n",
      "2 :   0.981\n",
      "3 :   0.9817\n",
      "4 :   0.9822\n",
      "5 :   0.9826\n",
      "6 :   0.9812\n",
      "7 :   0.9808\n",
      "8 :   0.9817\n",
      "9 :   0.98\n",
      "10 :   0.9803\n",
      "11 :   0.9805\n",
      "12 :   0.9798\n",
      "13 :   0.98\n",
      "14 :   0.981\n",
      "15 :   0.9794\n",
      "16 :   0.9805\n",
      "17 :   0.9804\n",
      "18 :   0.9805\n",
      "19 :   0.9794\n",
      "20 :   0.9805\n",
      "21 :   0.9815\n",
      "22 :   0.9793\n",
      "23 :   0.9807\n",
      "24 :   0.9797\n",
      "25 :   0.9817\n",
      "26 :   0.98\n",
      "27 :   0.9808\n",
      "28 :   0.9798\n",
      "29 :   0.9796\n",
      "30 :   0.9807\n",
      "31 :   0.98\n",
      "32 :   0.9794\n",
      "33 :   0.9817\n",
      "34 :   0.9817\n",
      "35 :   0.9804\n",
      "36 :   0.9805\n",
      "37 :   0.9826\n",
      "38 :   0.9786\n",
      "39 :   0.9807\n",
      "40 :   0.9794\n",
      "41 :   0.9819\n",
      "42 :   0.9796\n",
      "43 :   0.9803\n",
      "44 :   0.9805\n",
      "45 :   0.9809\n",
      "46 :   0.9797\n",
      "47 :   0.9812\n",
      "48 :   0.9811\n",
      "49 :   0.9809\n",
      "Accuracy is:     0.9809\n",
      "Confidence is:     0.983138875806\n",
      "l1coef = 0.000100, l2coef = 0.000000\n",
      "Training MNIST data...\n",
      "0 :   0.9732\n",
      "1 :   0.9708\n",
      "2 :   0.9721\n",
      "3 :   0.9657\n",
      "4 :   0.9708\n",
      "5 :   0.9702\n",
      "6 :   0.9698\n",
      "7 :   0.9696\n",
      "8 :   0.9689\n",
      "9 :   0.9721\n",
      "10 :   0.9674\n",
      "11 :   0.9715\n",
      "12 :   0.9728\n",
      "13 :   0.9696\n",
      "14 :   0.9703\n",
      "15 :   0.97\n",
      "16 :   0.9698\n",
      "17 :   0.9735\n",
      "18 :   0.9728\n",
      "19 :   0.9673\n",
      "20 :   0.97\n",
      "21 :   0.9663\n",
      "22 :   0.966\n",
      "23 :   0.972\n",
      "24 :   0.9704\n",
      "25 :   0.9692\n",
      "26 :   0.972\n",
      "27 :   0.9727\n",
      "28 :   0.9711\n",
      "29 :   0.9724\n",
      "30 :   0.9714\n",
      "31 :   0.969\n",
      "32 :   0.9677\n",
      "33 :   0.9715\n",
      "34 :   0.9689\n",
      "35 :   0.9718\n",
      "36 :   0.9723\n",
      "37 :   0.9714\n",
      "38 :   0.9727\n",
      "39 :   0.9682\n",
      "40 :   0.9713\n",
      "41 :   0.9669\n",
      "42 :   0.9684\n",
      "43 :   0.9721\n",
      "44 :   0.9722\n",
      "45 :   0.9734\n",
      "46 :   0.9729\n",
      "47 :   0.9707\n",
      "48 :   0.9728\n",
      "49 :   0.9707\n",
      "Accuracy is:     0.9707\n",
      "Confidence is:     0.969473694889\n",
      "l1coef = 0.000100, l2coef = 0.000010\n",
      "Training MNIST data...\n",
      "0 :   0.9737\n",
      "1 :   0.9735\n",
      "2 :   0.9715\n",
      "3 :   0.9692\n",
      "4 :   0.9703\n",
      "5 :   0.9704\n",
      "6 :   0.972\n",
      "7 :   0.9682\n",
      "8 :   0.9713\n",
      "9 :   0.9721\n",
      "10 :   0.9701\n",
      "11 :   0.9741\n",
      "12 :   0.9712\n",
      "13 :   0.9709\n",
      "14 :   0.9684\n",
      "15 :   0.9715\n",
      "16 :   0.971\n",
      "17 :   0.9725\n",
      "18 :   0.9714\n",
      "19 :   0.971\n",
      "20 :   0.9729\n",
      "21 :   0.967\n",
      "22 :   0.9695\n",
      "23 :   0.9728\n",
      "24 :   0.9753\n",
      "25 :   0.9717\n",
      "26 :   0.9707\n",
      "27 :   0.9727\n",
      "28 :   0.9692\n",
      "29 :   0.9698\n",
      "30 :   0.9738\n",
      "31 :   0.9701\n",
      "32 :   0.9731\n",
      "33 :   0.9691\n",
      "34 :   0.9709\n",
      "35 :   0.9732\n",
      "36 :   0.9724\n",
      "37 :   0.9666\n",
      "38 :   0.9706\n",
      "39 :   0.9711\n",
      "40 :   0.9731\n",
      "41 :   0.9723\n",
      "42 :   0.9691\n",
      "43 :   0.9725\n",
      "44 :   0.9696\n",
      "45 :   0.9706\n",
      "46 :   0.971\n",
      "47 :   0.9694\n",
      "48 :   0.9695\n",
      "49 :   0.971\n",
      "Accuracy is:     0.971\n",
      "Confidence is:     0.968142374768\n",
      "l1coef = 0.000100, l2coef = 0.000100\n",
      "Training MNIST data...\n",
      "0 :   0.9703\n",
      "1 :   0.9719\n",
      "2 :   0.9707\n",
      "3 :   0.9693\n",
      "4 :   0.9681\n",
      "5 :   0.972\n",
      "6 :   0.9705\n",
      "7 :   0.9704\n",
      "8 :   0.969\n",
      "9 :   0.9702\n",
      "10 :   0.9699\n",
      "11 :   0.9696\n",
      "12 :   0.9688\n",
      "13 :   0.9685\n",
      "14 :   0.9715\n",
      "15 :   0.9699\n",
      "16 :   0.9719\n",
      "17 :   0.9698\n",
      "18 :   0.973\n",
      "19 :   0.9665\n",
      "20 :   0.9692\n",
      "21 :   0.9714\n",
      "22 :   0.969\n",
      "23 :   0.9703\n",
      "24 :   0.9683\n",
      "25 :   0.9726\n",
      "26 :   0.9694\n",
      "27 :   0.9696\n",
      "28 :   0.9688\n",
      "29 :   0.9703\n",
      "30 :   0.9683\n",
      "31 :   0.9697\n",
      "32 :   0.9698\n",
      "33 :   0.9662\n",
      "34 :   0.971\n",
      "35 :   0.9712\n",
      "36 :   0.9678\n",
      "37 :   0.9688\n",
      "38 :   0.9668\n",
      "39 :   0.9678\n",
      "40 :   0.9712\n",
      "41 :   0.9661\n",
      "42 :   0.9702\n",
      "43 :   0.9712\n",
      "44 :   0.9666\n",
      "45 :   0.9678\n",
      "46 :   0.9692\n",
      "47 :   0.9713\n",
      "48 :   0.9677\n",
      "49 :   0.9706\n",
      "Accuracy is:     0.9706\n",
      "Confidence is:     0.96576061966\n"
     ]
    }
   ],
   "source": [
    "#l1 and l2 coefficients & Training Data\n",
    "#=================== Parameters to chnge ===============================#\n",
    "l1coef = [0.0,0.00001,0.0001]\n",
    "l2coef = [0.0,0.00001,0.0001]\n",
    "#=======================================================================#\n",
    "\n",
    "for i in l1coef:\n",
    "    for j in l2coef:\n",
    "        print(\"l1coef = %f, l2coef = %f\" %(i,j))\n",
    "        cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y)) + i * l1 + j * l2\n",
    "        updates = RMSprop(cost, params, lr=0.001)\n",
    "\n",
    "        # Define train and predict theano functions\n",
    "        train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "        predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "        predict_conf = theano.function(inputs=[X], outputs=y_x1, allow_input_downcast=True)\n",
    "        print('Training MNIST data...')\n",
    "        if TRAINING:\n",
    "            # Train in 50 epochs\n",
    "            for k in range(50):\n",
    "                # Select minibatch and train\n",
    "                for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)):\n",
    "                    cost = train(trX[start:end], trY[start:end])\n",
    "                # Show test set accuracy. Its cost is not used for optimization,\n",
    "                # it is just to show progress.\n",
    "                print(k, ':  ', np.mean(np.argmax(teY, axis=1) == predict(teX)))\n",
    "                # In each step save the learned weights\n",
    "                with open('LearnedParamsL1.model','wb') as fp:\n",
    "                    cPickle.dump(params,fp)\n",
    "            print(\"Accuracy is:    \",np.mean(np.argmax(teY, axis=1) == predict(teX)))\n",
    "            print(\"Confidence is:    \", np.mean(predict_conf(teX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
